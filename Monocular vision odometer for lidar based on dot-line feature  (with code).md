Abstract: In this paper, a novel method of laser radar + monocular vision odometry using points and lines is introduced. Compared with the previous lidar + vision odometry, more environmental structure information is utilized by introducing point and line features into pose estimation. A robust depth extraction method of point and line features is proposed, and the extracted depth value is used as a priori factor of point and line bundle adjustment method. The method greatly reduces the three-dimensional ambiguity of features and improves the accuracy of pose estimation. In addition, a pure visual motion tracking method and a new scale correction scheme are proposed, thus realizing an efficient and high-precision monocular vision odometry system. Evaluation of the publicly available KITTI dataset shows that this method achieves more accurate pose estimation than state-of-the-art methods, and sometimes even better than those that utilize semantic information. Related content and introduction, Lidar vision odometry has become an active research topic due to its wide application in robotics, virtual reality, autonomous driving, and other fields. Combining vision sensors and Lidar sensors as Lidar vision odometers realizes the advantages of both sensors. Therefore, research in computer vision, computer graphics, robotics, and other fields has received increasing attention. Relevant solutions include LOAM, LIMO, DVL-SLAM, ORB-SLAM, DSO, and more. In this paper, a robust and effective method for monocular vision odometry of LIDAR is proposed. It combines the features of points and lines in a purely geometric manner to extract more structural information from the scene environment than a single feature point system. More specifically, our system fuses the point and line features during camera tracking as landmarks, and uses the reprojection error of the point and line-based landmarks as a factor for back-end beam adjustment. During sensor fusion, a robust method is proposed to extract the depth of points and lines from LIDAR data and utilize this depth to assist camera tracking. This avoids the creation of three-dimensional landmarks based solely on possible ambiguous three-dimensional triangulation, especially for three-dimensional straight lines. In the point-line bundle adjustment, the depth prior is used as the prior factor to further improve the accuracy of attitude estimation. 

 ![avatar]( 20210113180327394.png) 

 Content essence, pre-processing, given a monocular image sequence and a lidar sequence, assume that the internal and external parameters of the two sensors have been calibrated, and the data of the two sensors have been time aligned. Set the local coordinates of the camera to the body coordinates, and the world coordinates to the starting point of the body coordinates. The figure above shows the framework of our system, which contains three running threads: the motion tracking thread (front end), the bundle adjustment thread (back end), and the loop closure thread. The front end first extracts point and line features in each frame, then estimates the depth of the features in each key frame, and finally estimates the camera pose using the inter-frame odometer. Perform scale correction to optimize the scale drift of the inter-frame odometer. The back end uses the dot-line constraint factor to adjust the dot-line bundle set. And based on the loop closure detection with point and line characteristic word bags, the pose of the key frame is further refined. 

 ![avatar]( 2021011318041327.png) 

 A. Feature extraction, which can use various point features (SIFT, SURF, ORB, etc.) as tracking features. To improve efficiency, ORB features are adopted here as point features, as described in ORB-SLAM2. During the detection process, ORB features are required to be distributed as evenly as possible in the image. Line Features For each image, the popular line feature detector, line segment detector (LSD), is used to detect the line segment and calculate the descriptor (lineband Descriptor, LBD) of the extracted line. B. Point and line depth extraction, in this section will introduce a method for extracting point and line depths from lidar data. Here, the depth of the 2D point feature refers to the depth of its corresponding 3D point, and the depth of the 2D line feature refers to the depth of the 3D landmark corresponding to the two endpoints. C. Frame-to-frame odometry, which uses a pure visual interframe odometer to estimate the camera pose of each frame, which is more effective than other ICP-based odometers such as V-LOAM. D. Optimization-based scale correction. Since the estimated scale may deviate from its actual physical scale, scale correction optimization is proposed here. Its core idea is to fuse the relative camera pose calculated by the ICP alignment step, and adjust the newly estimated key frame camera pose and related 3D landmarks through proportional correction optimization. 

 ![avatar]( 20210113180548128.png) 

  E. Dot-line bundle set adjustment, which forms a point-line bundle set adjustment at the back end between key frames of sliding adjacent windows, similar to the method in ORB-SLAM2. F. Loop detection, in the motion estimation process, loop closure includes loop detection and loop correction based on key frames. For loop detection, bags of words for point features (ORB descriptors) and line features (LBD descriptors) are first trained separately using the DBoW algorithm. Each key frame is then converted to a point vector and a line vector. Loop correction is performed when evaluating the similarity between key frames. 

 ![avatar]( 20210113180717185.png) 

 Experiments, the system is implemented based on ORB-SLAM2 with three threads, namely inter-frame odometry, key-frame-based bundle adjustment, and loopback detection. When performing ICP calibration, we use the Normal Distribution Transform (NDT) implemented in the PCL library to calculate the relative camera pose between two adjacent point clouds. We test our method on a publicly available KITTI training dataset containing 00-10 sequences with ground-truth trajectories, and analyze the accuracy and time efficiency of the method on a computer with 64-bit Linux operating system's Inte Core i7-2600@2.6GHz and 8G of memory. To summarize, an accurate and efficient method of LiDAR + visual odometry utilizing point and line features is presented in this paper. By utilizing more structural information, we demonstrate that our method is more accurate than pure geometric techniques and achieves comparable accuracy with systems using additional semantic information such as LIMO. We hope this work will shed light on future work, such as exploring other types of structural prior information, such as plane prior, parallel, orthogonal, or coplanar rules, to obtain more accurate LIDAR + camera sensor fusion systems 

