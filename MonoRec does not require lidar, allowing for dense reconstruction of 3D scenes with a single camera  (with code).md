Abstract, In this paper, we present MonoRec, a semi-supervised monocular dense reconstruction architecture that predicts depth maps in a dynamic environment based on a single moving camera. MonoRec presents a novel multi-stage training scheme that can eliminate the need for a semi-supervised loss formula for LiDAR depth values. MonoRec is carefully evaluated on the KITTI dataset and shows that it has state-of-the-art performance compared to multi-view and single-view approaches. With the model trained on KITTI, we further demonstrate that MonoRec generalizes well to the Oxford RobotCar dataset and the more challenging TUM-Mono dataset recorded by hand-held cameras 

 ![avatar]( 20210113174749723.png) 

 Related work and main contribution, Multi-view Stereo Vision (MVS) method estimates a dense point cloud of a 3D environment based on a set of images with known poses. In the past few years, a number of schemes have been developed based on classical optimization methods to solve the MVS problem. Based on monocular depth prediction depends only on a single image, and monocular depth prediction usually still consumes video sequences or stereoscopic images during training. Its aim is to solve a similar problem to the one presented in this paper, namely dense point cloud reconstruction of 3D scenes including static and dynamic objects. To combine the advantages of MVS with depth and monocular depth prediction, we propose MonoRec, a novel monocular dense reconstruction architecture consisting of MaskModule and DepthModule. Information from multiple consecutive images is encoded using cost quantities that are based on Structural Similarity Indicator Measures (SSIM) rather than based on the sum of absolute differences (SAD) as in previous work. The MaskModule is able to identify moving pixels and reduce the corresponding voxels in the cost quantities. As a result, MonoRec is not affected by artifacts on moving objects compared to other MVS methods, and therefore provides accurate depth estimates for both static and dynamic objects. With the proposed multi-stage training scheme, MonoRec can achieve state-of-the-art performance compared to other MVS and monocular depth prediction methods on the KITTI dataset. The figure below shows that the dense point cloud generated by this method, MonoRec, can provide a high-quality densely reconstructed point cloud with a single moving camera. The figure shows an example of a large-scale outdoor point cloud reconstruction (KITTI dataset) by simply accumulating a predicted depth map. 

 ![avatar]( 20210113174858278.png) 

 Main content MonoRec uses a set of consecutive image frames and corresponding camera poses to predict a dense depth map for a given key frame. The MonoRec structure combines MaskModule and DepthModule. MaskModule predicts a moving object mask that improves depth accuracy and allows us to eliminate noise in 3D reconstruction. DepthModule predicts a depth map based on the cost of the mask. Experimental comparison between MonoRec and other methods in the KITTI test set. The "Dataset" column shows the training dataset used by the corresponding method. The evaluation results show that this method achieves the best performance overall. In summary, this paper simply MonoRec, a deep learning architecture, can estimate a dense point cloud after accurate 3D reconstruction with only a single moving camera. The paper first proposes to construct the cost volume using SSIM as a photometric measure. To deal with dynamic objects commonly found in outdoor scenes, a novel MaskModule is proposed, which can predict the moving object mask based on the input cost quantity. Using the predicted mask, using the proposed DepthModule is able to estimate the accurate depth of static and dynamic objects. In addition, we propose a novel multi-stage training scheme as well as a semi-supervised loss formula for training depth prediction. Taken together, MonoRec is able to qualitatively and quantitatively outperform the latest MVS and monocular depth prediction methods on KITTI, and performs well on Oxford RobotCar and TUM-Mono. This ability to recover an accurate 3D dense point cloud from a single moving camera will help establish the camera as a pilot sensor for intelligence systems. 

