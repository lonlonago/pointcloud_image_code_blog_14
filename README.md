Kimera's Semantic SLAM System for Real-Time Reconstruction 

 Kimera is a C++ implementation of a semantic SLAM system with real-time metrics, using sensors with cameras and IMU inertial navigation data to build a semantically labeled 3D mesh of the environment. Kinera supports an efficient modular open source solution with ROS running on the CPU. Contains four modules: Fast and accurate vision-inertial odometer VIO pipeline (Kimera-VIO) Robust pose-based graph optimization complete SLAM implementation (Kimera-RPGO) Single and multi-frame 3D mesh generator (Kimera-Mesher) Semantic label 3D mesh generator (Kimera-Semantics) github: https://github.com/MIT-SPARK/Kimera 

 abstract 

 ![avatar]( 2020060222303419.JPG) 

 The paper provides an open-source C++ library for measuring semantic visual inertial simultaneous localization and composition systems (SLAM) in real time. The library goes beyond existing visual and visual inertial SLAM libraries (e.g. ORB-SLAM, VINSMono, OKVIS, ROVIO) to implement mesh reconstruction and semantic labeling in a 3D environment. Kimera's design takes modularity into account, and it has four key components: a visual inertial odometry (VIO) module for fast and accurate state estimation, a robust attitude graph global trajectory estimation optimizer, a lightweight 3D mesh module for fast mesh reconstruction, and a dense 3D metric semantic reconstruction module. The modules can be run individually or in combination, so Kimera can easily be modified to function individually as a VIO or as a complete SLAM system. Kimera is ROS-based running in real-time on the CPU to generate a three-dimensional metric semantic grid from semantically labeled images, which can be obtained through modern deep learning methods. We hope that the flexibility, computational efficiency, robustness, and accuracy provided by Kimera will provide a solid foundation for future metrics semantic SLAM and perception research, and allow researchers to upgrade their research across multiple domains (e.g. VIO, SLAM, 3D reconstruction, benchmarking and prototype development of their own work without the need to start from scratch). Scheme Comparison Kimeta is an open-source C++ library for real-time measurement of semantic SLAM. Figure (a) Visual inertial state estimation at IMU rates, and globally consistent robust trajectory estimation, computed (b) a scene low-latency local mesh that can be used for fast obstacle avoidance, and constructed (c) a globally semantically annotated 3D mesh that accurately reflects the ground true value model (d). 

 The main contents and contributions of Kimera are stereo images and inertial navigation measurements, and the outputs are: (1) efficient state estimation by IMU; (2) globally consistent trajectory estimation; (3) multiple environmental grids, including fast local grids and global grids based on semantic labels. 

 ![avatar]( 20200602223052846.png) 

  System block diagram 

 Kimera uses four threads to receive input and output (e.g. IMUs, image frames, and keyframes) at different frame rates. 

 (1) Kimera-VIO front end, which acquires stereoscopic images and IMU data. The front end performs online pre-integration to obtain a concise pre-integration measurement of the relative state between two consecutive key frames from the raw IMU data. The visual front end detects Shi-Tomasi corners, tracks them across frames using the Lukas-Kanade tracker, finds left and right stereo matches, and performs geometric verification. And outputs IMU measurements of characteristic trajectories and pre-integration. And publishes state estimates based on IMU frame rate. 

 (2) Kimera-VIO outputs optimized state estimation, and at each key frame, pre-integrated IMU and visual measurements are added to the constituent VIO backend. 

 查看原文视频 https://mp.weixin.qq.com/s?__biz=MzI0MDYxMDk0Ng==&tempkey=MTA2M19MOGRGVkhKTnk5Wk50OTRZcmctdzd2UExxb0dFUFB2WmU1aEVQLXBobjBDeTY0djI2eEI2eV9Ib2hMTVNnUjk5bGNsUDl3WF9vZ01PT0d3UllPSUpra1NzV2duT2VUVC1pWE51YlhEWVFnYmVRMmQzeVNoT09LSmZLZk9WaDAwRm9odVdFdFhGdzhmaUdLeW9oaFFveWQ0aC1MNElLRkFZMlQ3UUNBfn4%3D&chksm=691976735e6eff65cf3a2adab930efa3077c6fe718b68f6da44124dbc46bba0cc932f09040db#rd 

 (3) Pose map optimization Kimera-RPG, detecting the closed loop between the current key frame and the past key frame. Closed loop detection relies on the DBoW2 library, using bag-of-word notation to quickly detect the assumed closed loop. Closed loop can be detected, out-of-frame points can be eliminated, and globally consistent trajectories can be estimated. Two types of 3D meshes are quickly generated by Kimera-Mesher: single-frame mesh and multi-frame mesh (that is, the surface reconstruction after triangulation of the point cloud, etc.) 

 (4) Kimera-Semantics semantic labels, based on Kimera-VIO's pose estimation, using 2D semantically annotated images (generated at each key frame) to semantically annotate the global grid; 2D semantic labels can be obtained with off-the-shelf tools for pixel-level 2D semantic segmentation to obtain a refined semantic label grid. 

 ![avatar]( 2020060222325224.JPG) 

  The color difference between the true value and the true value 

 summarize 

 ![avatar]( 20200602223330909.JPG) 

 Kimeta is an open source C++ library for measurable semantic SLAM. It includes state-of-the-art visual odometry implementations, robust pose map optimization, mesh reconstruction, and 3D semantic markup. It runs in real time on the CPU and provides a continuous set of integration and benchmarking tools for those interested to research on their own.  



--------------------------------------------------------------------------------

![avatar]( 83607b7a381c4344a8d6bf190854ddb2.png) 

 1. Prepare the file 2011_09_30_calib.zip 2011_09_30_drive_0016_extract.zip 2011_09_30_drive_0016_sync.zip kitti2bag.py all related files of kitti can be private message me to send the link 2. Create a new folder 2011_09_30_calib, unzip the above files and combine them according to the following levels  

 3. Open end point in 2011_09_30_calib folder and execute 

  ```python  
After clicking on the GitHub Sponsor button above, you will obtain access permissions to my private code repository ( https://github.com/slowlon/my_code_bar ) to view this blog code. By searching the code number of this blog, you can find the code you need, code number is: 2024020309573725957
  ```  
 Result: 

  ```python  
After clicking on the GitHub Sponsor button above, you will obtain access permissions to my private code repository ( https://github.com/slowlon/my_code_bar ) to view this blog code. By searching the code number of this blog, you can find the code you need, code number is: 2024020309573725957
  ```  


--------------------------------------------------------------------------------

SLAM consists of two main tasks: localization and composition. In mobile robots or autonomous driving, this is a very important problem: for the robot to move accurately, it must have a map of the environment, so to build a map of the environment, it needs to know the robot's location. This series of articles is mainly divided into four parts: In the first part, we will introduce Lidar SLAM, including Lidar sensors, open source Lidar SLAM systems, deep learning in Lidar, and challenges and the future. The second part focuses on Visual SLAM, including camera sensors, open source visual SLAM systems for different dense SLAMs. The third part introduces visual inertial mileage method SLAM, deep learning in visual SLAM, and the future. In the fourth part, we will introduce the integration of lidar and vision. 

 In 1990, [1] the use of EKF (Extended Kalman Filter) was first proposed to gradually estimate the posterior distribution of robot pose and the position of landmarks. In fact, the robot starts from the unknown position of the unknown environment, locates its own position and attitude by repeatedly observing the environmental characteristics during the movement process, and then builds an incremental map of the surrounding environment according to its own pose, so as to achieve the purpose of simultaneous positioning and map construction. 

 In fact, the positioning problem is a very complex and hot issue in recent years. Positioning technology depends on the needs of the environment for cost, accuracy, positioning frequency and robustness, which can be achieved by GPS (Global Position System), IMU (Inertial Measurement Unit) and wireless signals, etc. [2]. But GPS can only work outdoors, and IMU systems have cumulative errors. Wireless technology, as an active system, cannot strike a balance between cost and accuracy. With rapid development, SLAM equipped with lidar, cameras, IMUs and other sensors has risen in recent years. Starting with filter-based SLAM, graph-based SLAM now plays a major role. The algorithm is derived from KF (Kalman Filter), EKF, and PF (Particle Filter) to graph-based optimization. And single-threading has been replaced by multi-threading. SLAM's technology has also transitioned from the earliest prototypes for military use to later multi-sensor fusion robotics applications. 

 LiDAR sensors, LiDAR sensors can be divided into 2D LiDAR and 3D LiDAR, which are defined by the number of light beams of LiDAR. In terms of production process, LiDAR can also be divided into mechanical LiDAR, hybrid solid-state LiDAR (such as MEMS) (micro-electromechanical) and solid-state LiDAR. Solid-state LiDAR can be produced by phased array and flash memory technology. Velodyne: In mechanical LiDAR, it has VLP-16, HDL-32E and HDL-64E. In hybrid solid-state LiDAR, it has 32E Ultra Puck Auto. Arguably the LiDAR with the most information and the most complete software. 

 SLAMTEC: It has low-cost lidar and robotics platforms, such as RPLIDAR A1, A2 and R3. Single-line lidar, is a good introductory lidar for laser SLAN, plus a mobile platform, you can make a mobile robot. Ouster: Mechanical lidar with 16 to 128 channels. Quanergy: S3 is the world's first released solid-state lidar, M8 is mechanical lidar. S3-QI is micro-solid-state lidar. Ibeo: It has Lux 4L and Lux 8L in mechanical lidar. In cooperation with Valeo, it released hybrid solid-state lidar, named Scala. The development trend of lidar is miniaturization and lightweight solid-state, lidar will occupy the market and be able to meet the application of most products. Other LiDAR companies include but are not limited to Sick, Hokuyo, HESAI, RoboSense, LeddarTech, ISureStar, benewake, Livox, Innovusion, Innoviz, Trimble, Leishen Intelligent Systems 

 2D LiDAR SLAM 

 • Gmapping: It is the most used SLAM package among robots based on the RBPF (Rao-Blackwellisation Local Filter) method. It adds a scan matching method to estimate the position [3]. It is an improved version of the base FastSLAM [4] with raster maps. call relationships between major functions in gmapping 

 • HectorSlam: It combines a 2D SLAM system and 3D navigation with scan matching technology and inertial sensing systems [5]. • KartoSLAM: It is a graph-based SLAM system [6]. • LagoSLAM: Its foundation is graph-based SLAM, which is a way to minimize non-linear non-convex cost functions [7]. • CoreSLAm: It is an algorithm that is understandable with minimal performance loss [8]. • Cartographer: This is Google's SLAM system [9]. It employs submaps and closed-loop detection to achieve better product-level performance. The algorithm can be configured across multiple platforms and sensors to deliver SLAM in 2D and 3D. 

 3D LiDAR SLAM 

 • Loam: This is a method of building a map in real time using 3D Lidar [10] for state estimation. It also has a back-and-forth rotating version (which should refer to the way Lidar scans) and a continuous scanning 2D Lidar version. • Lego-Loam: It inputs point clouds from Vdyelone VLP-16 Lidar (placed horizontally) and optional IMU data as input. The system outputs 6D attitude estimation in real time and has global optimization and closed-loop detection [11]. • Cartographer: It supports 2D and 3D SLAM [9]. • IMLS-SLAM: It proposes a new low-drift SLAM algorithm based only on 3D LiDAR data based on a scanning model matching framework [10]. 

 Laser SLAM based on deep learning 

 Detection of feature-based deep learning: PointNetVLAD [11] allows end-to-end training to extract global descriptors from a given 3D point cloud to solve point cloud-based location recognition retrieval. VoxelNet [12] is a general-purpose 3D detection network that unifies feature extraction and bounding box prediction into a single-stage, end-to-end trainable deep network. Other work can be seen in BirdNet [13]. LMNet [14] describes an efficient single-stage Deep Convolutional Neural Network for detecting objects and outputting object graphs and bounding box offset values for each point. PIXOR [15] is a no-proposal single-stage detector that outputs directional 3D object estimates decoded from pixel-level neural networks predictions. Yolo3D [16] builds on the success of oneshot regression meta-architecture in 2D perspective image space and extends it to generate directional 3D object bounding boxes from LiDAR point clouds. PointCNN [17] suggests learning the X transform from an input point cloud. The X transform is applied via element-by-element product and summation operations of a typical convolution operator. MV3D [18] is a sensory fusion framework that takes a lidar point cloud and an RGB image as input and predicts an oriented 3D bounding box. PU-GAN [19] proposes a new point cloud upsampling network based on generative adversarial networks (GANs). 

 Segmentation and recognition of point clouds: The methods for segmentation of 3D point clouds can be divided into edge-based methods, region growth, model fitting, hybrid methods, machine learning applications, and deep learning [20]. This paper focuses on the methods of deep learning. PointNet [21] designs a new type of neural networks that are directly input to point clouds, which has the functions of classification, segmentation, and semantic analysis. PointNet ++ [22] learns the hierarchical characteristics that have as the context scale increases on the basis of PointNet. In an end-to-end 3D object detection network based on PointNet ++. VoteNet [23] constructs a 3D detection flow for point clouds. SegMap [24] is a map representation solution for localization and mapping problems based on line segment extraction in 3D point clouds. SqueezeSeg [25] is a convolutional neural networks with recursive CRF (conditional random field) for segmenting road targets in real time from a 3d lidar point cloud. PointSIFT [26] is a semantic segmentation framework for 3D point clouds. It is based on a simple module that extracts features from adjacent points in eight directions. PointWise [27] proposes a convolutional neural networks for semantic segmentation and object recognition using 3D point clouds. 3P-RNN [28] is a novel end-to-end approach for semantic segmentation of unstructured point clouds along two horizontal directions to take advantage of inherent contextual features. Other similar work can be seen, but not limited to SPG [29] and Review [30]. SegMatch [31] is a 3D-based closed-loop approach to segmentation detection and matching. KdNetwork [32] is designed for 3D model recognition tasks and can be used with unstructured point clouds. DeepTemporalSeg [33] proposes a Deep Convolutional Neural Network (DCNN) for semantic segmentation of LiDAR scans that are time-consistent. LU-Net [34] implements the function of semantic segmentation instead of applying some global 3D segmentation methods. 

 Point cloud positioning: 

 The paper [35] is a novel learning-based LiDAR localization system that can achieve centimeter-level localization accuracy. SuMa ++ [36] calculates semantic segmentation results in a point-labeled manner throughout the scanning process, allowing us to construct semantically rich maps with labeled surfels and improve projection scan matching through semantic constraints 

 The challenges and future of point cloud SLAM 

 1) Cost and adaptability The advantage of Lidar is that it can provide 3D information and is not affected by changes in luminous light. In addition, the viewing angle is relatively large and can reach 360 degrees. However, the technical threshold of Lidar is very high, resulting in a long development cycle and high cost. In the future, miniaturization, reasonable cost, solid state and achieving high reliability and adaptability are the trends. 2) Low texture and dynamic environment, most SLAM systems can only work in a fixed environment, but the environment is constantly changing. In addition, low texture environment (such as long corridors and large pipes) will cause trouble for Lidar SLAM. [37] Use IMU to assist 2D SLAM to solve the above obstacles. In addition, [38] the time dimension is incorporated into the process of building the map to enable the robot to maintain an accurate map while operating in a dynamic environment. More in-depth consideration should be given to how to make the Lidar SLAM more powerful for low-texture and dynamic environments, and how to keep the map up to date. 3) Adversarial sensor attack deep neural networks are vulnerable to adversarial samples, which has also been demonstrated in camera-based perception. However, in Lidar-based perception, it is very important but has not been explored. [39] By relay attack, the Lidar is first tricked, interfering with the output data and distance estimates. This novel saturation attack is completely unable to make the Lidar sense a certain direction based on the Velodynes VLP-16. [ 40] The possibility of strategically controlling deceptive attacks to deceive machine learning models is explored. This paper treats the task as an optimization problem, and designs a modeling method for the input perturbation function and the objective function to increase the attack success rate to around 75%. Adversarial sensor attacks will deceive SLAM systems based on lidar point clouds, which are almost difficult to detect and defend against, and therefore invisible. In this case, research on how to prevent lidar SLAM systems from being attacked by adversarial sensors should become a new topic. 

 References 

 [1] Randall Smith, Matthew Self, and Peter Cheeseman. Estimating uncertain spatial relationships in robotics. In Autonomous robot vehicles, pages 167–193. Springer, 1990. 

 [2] Baichuan Huang, Jingbin Liu, Wei Sun, and Fan Yang. A robust indoor positioning method based on bluetooth low energy with separate channel information. Sensors, 19(16):3487, 2019. 

 [3] Sebastian Thrun, Wolfram Burgard, and Dieter Fox. Probabilistic robotics. MIT press, 2005. 

 [4] Michael Montemerlo, Sebastian Thrun, Daphne Koller, Ben Wegbreit, et al. Fastslam 2.0: An improved particle filtering algorithm for simultaneous localization and mapping that provably converges. In IJCAI, pages 1151–1156, 2003. [5] Stefan Kohlbrecher, Oskar Von Stryk, Johannes Meyer, and Uwe Klingauf. A flexible and scalable slam system with full 3d motion estimation. In 2011 IEEE International Symposium on Safety, Security, and Rescue Robotics, pages 155–160. IEEE, 2011. 

 [6] Kurt Konolige, Giorgio Grisetti, Rainer K ¨ummerle, Wolfram Burgard, Benson Limketkai, and Regis Vincent. Efficient sparse pose adjustment for 2d mapping. In 2010 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 22–29. IEEE, 2010. 

 [7] Luca Carlone, Rosario Aragues, Jos´e A Castellanos, and Basilio Bona. A linear approximation for graph-based simultaneous localization and mapping. Robotics: Science and Systems VII, pages 41–48, 2012. 

 [8] B Steux and O TinySLAM El Hamzaoui. A slam algorithm in less than 200 lines c-language program. Proceedings of the Control Automation Robotics & Vision (ICARCV), Singapore, pages 7–10, 2010. 

 [9] Wolfgang Hess, Damon Kohler, Holger Rapp, and Daniel Andor. Realtime loop closure in 2d lidar slam. In 2016 IEEE International Conference on Robotics and Automation (ICRA), pages 1271–1278. IEEE, 2016. 

 [10] Jean-Emmanuel Deschaud. Imls-slam: scan-to-model matching based on 3d data. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 2480–2485. IEEE, 2018. 

 [11] Mikaela Angelina Uy and Gim Hee Lee. Pointnetvlad: Deep point cloud based retrieval for large-scale place recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4470–4479, 2018. 

 [12] Yin Zhou and Oncel Tuzel. Voxelnet: End-to-end learning for point cloud based 3d object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4490–4499, 2018. 

 [13] Jorge Beltr´an, Carlos Guindel, Francisco Miguel Moreno, Daniel Cruzado, Fernando Garcia, and Arturo De La Escalera. Birdnet: a 3d object detection framework from lidar information. In 2018 21st International Conference on Intelligent Transportation Systems (ITSC), pages 3517–3523. IEEE, 2018. 

 [14] Kazuki Minemura, Hengfui Liau, Abraham Monrroy, and Shinpei Kato. Lmnet: Real-time multiclass object detection on cpu using 3d lidar. 

 [15] Bin Yang, Wenjie Luo, and Raquel Urtasun. Pixor: Real-time 3d object detection from point clouds. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 7652–7660, 2018. 

 [16] Waleed Ali, Sherif Abdelkarim, Mahmoud Zidan, Mohamed Zahran, and Ahmad El Sallab. Yolo3d: End-to-end real-time 3d oriented object bounding box detection from lidar point cloud. In Proceedings of the European Conference on Computer Vision (ECCV), pages 0–0, 2018. 

 [17] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. Pointcnn: Convolution on x-transformed points. In Advances in Neural Information Processing Systems, pages 820–830, 2018. 

 [18] Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia. Multi-view 3d object detection network for autonomous driving. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1907–1915, 2017. 

 [19] Ruihui Li, Xianzhi Li, Chi-Wing Fu, Daniel Cohen-Or, and PhengAnn Heng. Pu-gan: A point cloud upsampling adversarial network. In Proceedings of the IEEE International Conference on Computer Vision, pages 7203–7212, 2019. 

 [20] E Grilli, F Menna, and F Remondino. A review of point clouds segmentation and classification algorithms. The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences, 42:339, 2017. 

 [21] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. arXiv preprint arXiv:1612.00593, 2016. 

 [22] Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarc 

 hical feature learning on point sets in a metric space. arXiv preprint arXiv:1706.02413, 2017. 

 [23] Charles R Qi, Or Litany, Kaiming He, and Leonidas J Guibas. Deep hough voting for 3d object detection in point clouds. arXiv preprint arXiv:1904.09664, 2019. [24] Renaud Dube, Andrei Cramariuc, Daniel Dugas, Juan Nieto, Roland Siegwart, and Cesar Cadena. SegMap: 3d segment mapping using data-driven descriptors. In Robotics: Science and Systems (RSS), 2018. 

 [25] Bichen Wu, Alvin Wan, Xiangyu Yue, and Kurt Keutzer. Squeezeseg: Convolutional neural nets with recurrent crf for real-time road-object segmentation from 3d lidar point cloud. ICRA, 2018. 

 [26] Mingyang Jiang, Yiran Wu, Tianqi Zhao, Zelin Zhao, and Cewu Lu. Pointsift: A sift-like network module for 3d point cloud semantic segmentation. arXiv preprint arXiv:1807.00652, 2018. 

 [27] Binh-Son Hua, Minh-Khoi Tran, and Sai-Kit Yeung. Pointwise convolutional neural networks. In Computer Vision and Pattern Recognition (CVPR), 2018. 

 [28] Xiaoqing Ye, Jiamao Li, Hexiao Huang, Liang Du, and Xiaolin Zhang. 3d recurrent neural networks with context fusion for point cloud semantic segmentation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 403–417, 2018. 

 [29] Loic Landrieu and Martin Simonovsky. Large-scale point cloud semantic segmentation with superpoint graphs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4558–4567, 2018. 

 [30] Renaud Dub´e, Daniel Dugas, Elena Stumm, Juan Nieto, Roland Siegwart, and Cesar Cadena. Segmatch: Segment based place recognition in 3d point clouds. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 5266–5272. IEEE, 2017. 

 [31] Roman Klokov and Victor Lempitsky. Escape from cells: Deep kdnetworks for the recognition of 3d point cloud models. In Proceedings of the IEEE International Conference on Computer Vision, pages 863– 872, 2017. 

 [32] Ayush Dewan and Wolfram Burgard. Deeptemporalseg: Temporally consistent semantic segmentation of 3d lidar scans. arXiv preprint arXiv:1906.06962, 2019. [33] Pierre Biasutti, Vincent Lepetit, Jean-Franois Aujol, Mathieu Brdif, and Aurlie Bugeau. Lu-net: An efficient network for 3d lidar point cloud semantic segmentation based on end-to-end-learned 3d features and u-net. 08 2019. 

 [34] Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. Pointrcnn: 3d object proposal generation and detection from point cloud. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–779, 2019. 

 [35] Lu Weixin, Zhou Yao, Wan Guowei, Hou Shenhua, and Song Shiyu. L3-net: Towards learning based lidar localization for autonomous driving. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 

 [36] Chen Xieyuanli, Milioto Andres, and Emanuelea Palazzolo. Suma++: Efficient lidar-based semantic slam. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2019. 

 [37] Zhongli Wang, Yan Chen, Yue Mei, Kuo Yang, and Baigen Cai. Imuassisted 2d slam method for low-texture and dynamic environments. Applied Sciences, 8(12):2534, 2018. 

 [38] Aisha Walcott-Bryant, Michael Kaess, Hordur Johannsson, and John J Leonard. Dynamic pose graph slam: Long-term mapping in low dynamic environments. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 1871–1878. IEEE, 2012. 

 [39] Hocheol Shin, Dohyun Kim, Yujin Kwon, and Yongdae Kim. Illusion and dazzle: Adversarial optical channel exploits against lidars for automotive applications. In International Conference on Cryptographic Hardware and Embedded Systems, pages 445–467. Springer, 2017. 

 [40] Yulong Cao, Chaowei Xiao, Benjamin Cyr, Yimeng Zhou, Won Park, Sara Rampazzi, Qi Alfred Chen, Kevin Fu, and Z Morley Mao. Adversarial sensor attack on lidar-based perception in autonomous driving. arXiv preprint arXiv:1907.06826, 2019 



--------------------------------------------------------------------------------

#  Grid data is sampled as a point cloud 

 ![avatar]( daf36cd3f8144aa983de23bfe859711d.gif) 

##  First, the effect display in VS 

##  Integration into QT 

 ![avatar]( fba4df48915d41618f1a3710a761fea6.gif) 

##  Third, cloudcompare implementation 

 ![avatar]( 57c65ac4f77f46368edbdf2af37700e1.png) 

 ![avatar]( e5c06174ad234a4a8a4853224db65866.png) 

 Sampling and comparison: red is algorithm collection, green is cloudcompare collection  

 ![avatar]( 732f81d27d4147c8b6d2b3373da711c8.png) 

 ![avatar]( c7b8c545ee0c48da8ab97df2a9cc06a1.png) 

##  Fourth, the qt software is uploaded to the resource 

 Download resources 



--------------------------------------------------------------------------------

#  foreword 

 I have read articles on grid parameterization in the past few days, and implemented the Least Squares Conformal Maps method 

 Refer to relevant articles, refer to relevant articles 

#  effect 

 ![avatar]( 5220a29d0ef5451b9e489748a19be0d2.gif) 



--------------------------------------------------------------------------------

Point cloud PCL free knowledge planet, point cloud paper speed reading. 

 文章：Monocular Camera Localization in Prior LiDAR Maps with 2D-3D Line Correspondences 

 作者：Huai Yu1;2, Weikun Zhen2, Wen Yang1, Ji Zhang2 and Sebastian Scherer 

 Compiler: Point Cloud PCL 

 Source: arXiv 2020 

 Welcome to join the free knowledge planet, get PDF papers, and welcome to forward Moments. The article is for academic sharing only. If there is any infringement, please contact to delete the article. Please do not reprint without the consent of the blogger. 

 The paper reading module will share articles related to point cloud processing, SLAM, 3D vision, and high-precision maps. Official account is dedicated to the sharing of dry goods related to understanding the field of 3D vision. Welcome to join me. We will read an article every day and start the sharing journey. If you are interested, please contact WeChat dianyunpcl@163.com. 

 abstract 

 At present, the odometry (VO & VIO) techniques of vision and vision + inertial navigation have been well developed in state estimation, but cumulative drift and pose jumping will inevitably occur when the loop is closed. To overcome these problems, we propose a monocular localization method based on 2D-3D line correspondence. In order to deal with the differences in data and models between LiDAR point clouds and images, 3D geometric lines are extracted offline from LiDAR maps, and robust 2D image straight lines are extracted in real time from video sequences. Possible 2D-3D line correspondence can be effectively obtained through the prediction of the pose of the VIO. Then the camera pose and 2D-3D correspondence are iteratively optimized by minimizing the projection error of the corresponding points and eliminating outliers. Experimental results on the EurocMav dataset and our acquired dataset demonstrate that the method can effectively estimate the camera pose in a structured environment without cumulative drift or pose jumps. 

 The main contribution of this work is a method for estimating pose using 2D-3D geometric line correspondence for monocular localization, efficiently correlating each key frame with a previous LiDAR map. The geometric lines are robust to cope with changes in appearance and are suitable for camera localization in urban environments. The image below shows the camera pose with 2D-3D line correspondence and estimation in a LiDAR map. 

 ![avatar]( 20210301213025282.gif) 

 This paper presents an application of a monocular positioning system in an existing road LiDAR map. The LIDAR map on the right is colored by height. The red and green trajectories are VINS Mono and our results, respectively. The upper left image shows a three-dimensional line projection (green) using the estimated VINS Mono pose (with occlusion) and the extracted two-dimensional line (red), while the lower left image uses the proposed method of pose estimation for the two-dimensional-three-dimensional line corresponding to the graph. 

 Main content 

 This method can simultaneously estimate the camera pose and 2D-3D line correspondence with six degrees of freedom. The camera pose is optimized by minimizing the 3D line reprojection error, while the accurate camera pose helps to weed out outliers. The 3D line features are extracted offline on a large-scale 3D LiDAR point cloud map as a preliminary work for real-time 2D-3D correspondence estimation. At the same time, the PnP solution gives the coarse pose initial value of the first frame for the manually labeled 2D-3D point correspondence. Then VINS-Mono is used to predict the camera's motion between adjacent key frames. Using the predicted pose, local 3D lines in the camera's field of view (FoV) are extracted and directly matched to the 2D lines extracted online from the image sequence. Finally, iteratively update the camera pose and 2D-3D correspondence. The process is as shown in the figure. 

 ![avatar]( 20210301213025284.gif) 

 Process of Localization in LIDAR Point Cloud Map Based on Monocular Camera 

 A. Line extraction in 2D-3D 

 In urban environments, geometric structures are usually represented by line segments and planes. Here, a segmentation-based three-dimensional straight line detection method is used to extract three-dimensional straight lines from a lidar point cloud map. The basic idea is to cluster the point clouds into flat areas and use contour fitting to obtain three-dimensional line segments. The method has good robustness and effectiveness for large-scale unorganized point clouds. Although it takes time to process millions of points, the 3D lines of all maps are extracted only once before we start tracking. 

 For the extraction of two-dimensional lines, it is necessary to extract important two-dimensional geometric lines that are consistent with three-dimensional lines and robust to noise. This is a challenge in urban scenes because a large amount of texture noise divides more two-dimensional line segments, and some geometric edges are invisible in two-dimensional images on uniformly colored structures such as white walls. Many state-of-the-art line segment detection (LSD) methods are proposed in computer vision algorithms, and traditional methods run efficiently on the CPU. However, the detected line segments are scattered and noisy, as shown in the left image. These scattered and noisy features generate a large number of 2D-3D matching outliers. Considering the integrity of the line and the robustness to noise, a learning-based LSD algorithm is adopted, which uses attraction field mapping (AFM) to transform the LSD problem into a color-based region growth problem. 

 ![avatar]( 16230e5b8872661c53f498aacfd5bfed.png) 

 Comparison of Different Methods for Line Segment Detection in 2D Images 

 B. Matching of 2D-3D line segments 

 For a single frame image, the main steps to obtain the 2D-3D correspondence include initial camera pose prediction, 3D line detection, and single 2D-3D line correspondence estimation. Here, the extraction of 3D lines in the camera FoV helps to improve efficiency because the local 3D lines in the FoV are very limited compared to all 3D lines in the 3D map. Considering that it is difficult to perform occlusion detection only on the 3D line map, all 3D lines are kept in the field of view without discarding the occlusion lines. 

 ![avatar]( dfeeabaabb8e34999a510eff0dba141d.png) 

 Check whether the three-dimensional line segment is in the field of view 

 • If both endpoints are in the field of view (Figure (a)), the entire three-dimensional line segment is maintained as a locally visible feature. 

 • When only one endpoint is in the field of view (Fig. (b)), we iteratively sample new 3D points on the 3D line from the viewpoint in a line length ratio of 0:1 and check the visibility of the new sampled points. The resulting set of 3D line segments with the longest length in the field of view is stored as a local visual feature. 

 • As shown in Figure (c), we can also sample points to extract a subset when both endpoints are not in the field of view, but a subset is in the field of view. However, most of the map line segments that are not visible are in this case, and the three-dimensional line segment with two endpoints out of field of view is abandoned for efficiency. 

 C. Line matching and pose optimization 

 For a single frame, the camera pose can be optimized by minimizing the point-to-infinity straight-line distance projected by the endpoints of the two 3D line segments to the corresponding 2D straight-line distance. The Lie algebra for estimating the camera pose Pt is expressed as < unk > t, and the coefficient vector for the infinitely long 2D line is H = abc. The objective function is to minimize the projection error between all 2D-3D counterparts: 

 ![avatar]( 20210301213025288.gif) 

 However, the robustness of a single frame of 2D-3D correspondence observations for real-time camera positioning is not sufficient. The 2D-3D correspondence relationship cannot constrain the six-degree-of-freedom pose when the 3D straight lines in the field of view are constrained or parallel in 3D space. Furthermore, even if the correspondence relationship is sufficient for pose estimation, the geometric positioning noise of the 2D and 3D straight lines can make the estimation unstable near the true pose. To address these issues, a sliding window is utilized to add more previous correspondence observations to optimize the current pose Pt (as shown) 

 ![avatar]( 706c161f0a1830f404e9787e2bbaaf74.png) 

  Position and Attitude Optimization Based on Sliding Window 

 experiment 

 The method was tested on two different real-world datasets. The first experiment was conducted on the publicly available EuRoC MAV dataset, which contains real ground trajectories. And, we experimented with the dataset acquired by the Realsense D435i camera under different conditions to verify the performance of the scheme. The test computer was an Intel core i7-4790K processor, 32GB of memory, and an Nvidia GeForce GTX 980Ti graphics processing unit. The GPU was only used for 2D line surveying. 

 ![avatar]( 2ea66ca106b4f7336b328722987485c1.png) 

 Camera positioning results on the EuRoC MAV dataset. The image in the upper left shows the 2D line segment (red) extracted using VINS-Mono and the projected 3D line (green) (with occlusion), and the 2D-3D correspondence using our method in the lower left. The right plot shows the result of the estimated trajectory (green) aligned with the true value (red) in the lidar map. 

 ![avatar]( 9a911b02d97656bb103d7793547be070.png) 

 Statistical results of running ATE RMSE 5 times on the EuRoC MAV dataset 

 ![avatar]( 20210301213025291.gif) 

 RPE (The average relative pose errors) RMSE statistical results for line segments of different lengths 

 ![avatar]( 20210301213025294.gif) 

 Relative trajectory error boxplot 

 To further evaluate our method in different environments, we tested our own datasets of indoor corridors and outdoor buildings acquired. The Intel RealSense D435i camera was used to acquire synchronized images and IMU data. The global shutter imager captured a monocular image sequence (640 × 480 pixels, 30Hz, IR projector turned off) with synchronized IMU data (200Hz). The lidar map was obtained by registering the FARO scanner focus3D S with several scan point cloud numbers, as shown in the figure. 

 ![avatar]( 2b0e431bea574e8df6ee2dce7b2fa983.png) 

 Three Scenarios LiDAR Point Cloud Map 

 ![avatar]( 7e2a42c3768974a8af339ae34d71709c.png) 

 The positioning result of the camera in an outdoor environment 

 ![avatar]( ce84b1aa7bf21698d7071c4ecf1aa8cd.png) 

 Positioning error in self-collected datasets 

 summarize 

 In this paper, a monocular camera localization method is proposed in a pre-constructed structured LiDAR point cloud map environment. This method utilizes the 3D geometric lines in the LiDAR map and the robust 2D lines detected in the image, and effectively obtains the coarse 2D-3D line correspondence based on the pose predicted by the camera motion of VINS-Mono. This 2D-3D correspondence optimization method can reduce the drift of the pose estimation of the VIO system without loop detection. And the qualitative and quantitative analysis results of the dataset show that the method can effectively obtain the reliable 2D-3D correspondence and accurate camera pose. 

 resource 

 3D point cloud paper and related application sharing 

 [Point cloud paper speed reading] Odometer based on lidar and positioning method in 3D point cloud map 

 3D object detection: MV3D-Net 

 Overview of 3D Point Cloud Segmentation (Part 1) 

 3D-MiniNet: Learning 2D Representations from Point Clouds for Fast and Efficient 3D LIDAR Semantic Segmentation (2020) 

 Use QT to add VTK plug-in under win to realize point cloud visualization GUI 

 JSNet: Joint Instances and Semantic Segmentation for 3D Point Clouds 

 A Survey of Semantic Segmentation of 3D Point Cloud in Large Scene 

 The outofcore module in PCL---Display of large-scale point clouds based on out-of-core octree 

 Target Segmentation Based on Local Convex 

 Point cloud labeling based on 3D convolutional neural networks 

 SuperVoxel of Point Cloud 

 Large-scale point cloud segmentation based on hyperdot graph 

 More articles can be viewed: A summary of historical articles on point cloud learning 

 SLAM and AR related sharing 

 [Open source solution sharing] ORB-SLAM3 is open source! 

 AVP-SLAM: Semantic SLAM in Automatic Parking Systems 

 [Point Cloud Paper Speed Reading] StructSLAM: Structured Line Feature SLAM 

 SLAM and AR Overview 

 Commonly used 3D depth cameras 

 Overview and Evaluation of Monocular Vision Inertial Navigation SLAM Algorithm for AR Devices 

 SLAM Review (4) Laser and Vision Fusion SLAM 

 Kimera's Semantic SLAM System for Real-Time Reconstruction 

 Overview of SLAM (3) - Vision and inertial navigation, vision and deep learning SLAM 

 Extensible SLAM Framework - OpenVSLAM 

 Gao Xiang: Challenges in unstructured road laser SLAM 

 SLAM Overview of Lidar SLAM 

 SLAM Method Based on Fisheye Camera 

 Online sharing and recording summary in the past 

 3D Model Retrieval Technology of the First Bilibili Recording 

 Application of deep learning in 3D scenes recorded and broadcast by Bilibili 

 The third Bilibili recording of CMake advanced learning 

 Point Cloud Object and Six-Degree-of-Freedom Attitude Estimation Recorded by Bilibili 

 The fifth Bilibili recording of point cloud deep learning semantic segmentation expansion 

 Pointnetlk Interpretation of the 6th Bilibili Recording 

 [线上分享录播]点云配准概述及其在激光SLAM中的应用 

 [线上分享录播]cloudcompare插件开发 

 [线上分享录播]基于点云数据的 Mesh重建与处理 

 [线上分享录播]机器人力反馈遥操作技术及机器人视觉分享 

 [线上分享录播]地面点云配准与机载点云航带平差 

 If you are interested in this article, please click "Original Reading" to get the QR code of Knowledge Planet. Be sure to join the free Knowledge Planet according to the remarks of "Name + School/Company + Research Direction", download the pdf document for free, and communicate with more friends who love to share! 

 If the above content is wrong, please leave a comment, and welcome to correct and communicate. If there is any infringement, please contact to delete it. 

 ![avatar]( db2ef668d6bb06e5962fc49405d55ba8.png) 

 Scan QR code 

                    Follow us 

 Let's share and learn together! Looking forward to friends who have ideas and are willing to share joining the free planet to inject fresh vitality of love and sharing. The topics shared include but are not limited to 3D vision, point clouds, high-precision maps, autonomous driving, and robotics and other related fields. 

 Sharing and cooperation: Group owner WeChat "920177957" (note required) Contact email: dianyunpcl@163.com, enterprises are welcome to contact the official account for cooperation. 

 Click "watching" and you will look better. 



--------------------------------------------------------------------------------

Abstract: In this paper, a novel method of laser radar + monocular vision odometry using points and lines is introduced. Compared with the previous lidar + vision odometry, more environmental structure information is utilized by introducing point and line features into pose estimation. A robust depth extraction method of point and line features is proposed, and the extracted depth value is used as a priori factor of point and line bundle adjustment method. The method greatly reduces the three-dimensional ambiguity of features and improves the accuracy of pose estimation. In addition, a pure visual motion tracking method and a new scale correction scheme are proposed, thus realizing an efficient and high-precision monocular vision odometry system. Evaluation of the publicly available KITTI dataset shows that this method achieves more accurate pose estimation than state-of-the-art methods, and sometimes even better than those that utilize semantic information. Related content and introduction, Lidar vision odometry has become an active research topic due to its wide application in robotics, virtual reality, autonomous driving, and other fields. Combining vision sensors and Lidar sensors as Lidar vision odometers realizes the advantages of both sensors. Therefore, research in computer vision, computer graphics, robotics, and other fields has received increasing attention. Relevant solutions include LOAM, LIMO, DVL-SLAM, ORB-SLAM, DSO, and more. In this paper, a robust and effective method for monocular vision odometry of LIDAR is proposed. It combines the features of points and lines in a purely geometric manner to extract more structural information from the scene environment than a single feature point system. More specifically, our system fuses the point and line features during camera tracking as landmarks, and uses the reprojection error of the point and line-based landmarks as a factor for back-end beam adjustment. During sensor fusion, a robust method is proposed to extract the depth of points and lines from LIDAR data and utilize this depth to assist camera tracking. This avoids the creation of three-dimensional landmarks based solely on possible ambiguous three-dimensional triangulation, especially for three-dimensional straight lines. In the point-line bundle adjustment, the depth prior is used as the prior factor to further improve the accuracy of attitude estimation. 

 ![avatar]( 20210113180327394.png) 

 Content essence, pre-processing, given a monocular image sequence and a lidar sequence, assume that the internal and external parameters of the two sensors have been calibrated, and the data of the two sensors have been time aligned. Set the local coordinates of the camera to the body coordinates, and the world coordinates to the starting point of the body coordinates. The figure above shows the framework of our system, which contains three running threads: the motion tracking thread (front end), the bundle adjustment thread (back end), and the loop closure thread. The front end first extracts point and line features in each frame, then estimates the depth of the features in each key frame, and finally estimates the camera pose using the inter-frame odometer. Perform scale correction to optimize the scale drift of the inter-frame odometer. The back end uses the dot-line constraint factor to adjust the dot-line bundle set. And based on the loop closure detection with point and line characteristic word bags, the pose of the key frame is further refined. 

 ![avatar]( 2021011318041327.png) 

 A. Feature extraction, which can use various point features (SIFT, SURF, ORB, etc.) as tracking features. To improve efficiency, ORB features are adopted here as point features, as described in ORB-SLAM2. During the detection process, ORB features are required to be distributed as evenly as possible in the image. Line Features For each image, the popular line feature detector, line segment detector (LSD), is used to detect the line segment and calculate the descriptor (lineband Descriptor, LBD) of the extracted line. B. Point and line depth extraction, in this section will introduce a method for extracting point and line depths from lidar data. Here, the depth of the 2D point feature refers to the depth of its corresponding 3D point, and the depth of the 2D line feature refers to the depth of the 3D landmark corresponding to the two endpoints. C. Frame-to-frame odometry, which uses a pure visual interframe odometer to estimate the camera pose of each frame, which is more effective than other ICP-based odometers such as V-LOAM. D. Optimization-based scale correction. Since the estimated scale may deviate from its actual physical scale, scale correction optimization is proposed here. Its core idea is to fuse the relative camera pose calculated by the ICP alignment step, and adjust the newly estimated key frame camera pose and related 3D landmarks through proportional correction optimization. 

 ![avatar]( 20210113180548128.png) 

  E. Dot-line bundle set adjustment, which forms a point-line bundle set adjustment at the back end between key frames of sliding adjacent windows, similar to the method in ORB-SLAM2. F. Loop detection, in the motion estimation process, loop closure includes loop detection and loop correction based on key frames. For loop detection, bags of words for point features (ORB descriptors) and line features (LBD descriptors) are first trained separately using the DBoW algorithm. Each key frame is then converted to a point vector and a line vector. Loop correction is performed when evaluating the similarity between key frames. 

 ![avatar]( 20210113180717185.png) 

 Experiments, the system is implemented based on ORB-SLAM2 with three threads, namely inter-frame odometry, key-frame-based bundle adjustment, and loopback detection. When performing ICP calibration, we use the Normal Distribution Transform (NDT) implemented in the PCL library to calculate the relative camera pose between two adjacent point clouds. We test our method on a publicly available KITTI training dataset containing 00-10 sequences with ground-truth trajectories, and analyze the accuracy and time efficiency of the method on a computer with 64-bit Linux operating system's Inte Core i7-2600@2.6GHz and 8G of memory. To summarize, an accurate and efficient method of LiDAR + visual odometry utilizing point and line features is presented in this paper. By utilizing more structural information, we demonstrate that our method is more accurate than pure geometric techniques and achieves comparable accuracy with systems using additional semantic information such as LIMO. We hope this work will shed light on future work, such as exploring other types of structural prior information, such as plane prior, parallel, orthogonal, or coplanar rules, to obtain more accurate LIDAR + camera sensor fusion systems 



--------------------------------------------------------------------------------

Abstract, In this paper, we present MonoRec, a semi-supervised monocular dense reconstruction architecture that predicts depth maps in a dynamic environment based on a single moving camera. MonoRec presents a novel multi-stage training scheme that can eliminate the need for a semi-supervised loss formula for LiDAR depth values. MonoRec is carefully evaluated on the KITTI dataset and shows that it has state-of-the-art performance compared to multi-view and single-view approaches. With the model trained on KITTI, we further demonstrate that MonoRec generalizes well to the Oxford RobotCar dataset and the more challenging TUM-Mono dataset recorded by hand-held cameras 

 ![avatar]( 20210113174749723.png) 

 Related work and main contribution, Multi-view Stereo Vision (MVS) method estimates a dense point cloud of a 3D environment based on a set of images with known poses. In the past few years, a number of schemes have been developed based on classical optimization methods to solve the MVS problem. Based on monocular depth prediction depends only on a single image, and monocular depth prediction usually still consumes video sequences or stereoscopic images during training. Its aim is to solve a similar problem to the one presented in this paper, namely dense point cloud reconstruction of 3D scenes including static and dynamic objects. To combine the advantages of MVS with depth and monocular depth prediction, we propose MonoRec, a novel monocular dense reconstruction architecture consisting of MaskModule and DepthModule. Information from multiple consecutive images is encoded using cost quantities that are based on Structural Similarity Indicator Measures (SSIM) rather than based on the sum of absolute differences (SAD) as in previous work. The MaskModule is able to identify moving pixels and reduce the corresponding voxels in the cost quantities. As a result, MonoRec is not affected by artifacts on moving objects compared to other MVS methods, and therefore provides accurate depth estimates for both static and dynamic objects. With the proposed multi-stage training scheme, MonoRec can achieve state-of-the-art performance compared to other MVS and monocular depth prediction methods on the KITTI dataset. The figure below shows that the dense point cloud generated by this method, MonoRec, can provide a high-quality densely reconstructed point cloud with a single moving camera. The figure shows an example of a large-scale outdoor point cloud reconstruction (KITTI dataset) by simply accumulating a predicted depth map. 

 ![avatar]( 20210113174858278.png) 

 Main content MonoRec uses a set of consecutive image frames and corresponding camera poses to predict a dense depth map for a given key frame. The MonoRec structure combines MaskModule and DepthModule. MaskModule predicts a moving object mask that improves depth accuracy and allows us to eliminate noise in 3D reconstruction. DepthModule predicts a depth map based on the cost of the mask. Experimental comparison between MonoRec and other methods in the KITTI test set. The "Dataset" column shows the training dataset used by the corresponding method. The evaluation results show that this method achieves the best performance overall. In summary, this paper simply MonoRec, a deep learning architecture, can estimate a dense point cloud after accurate 3D reconstruction with only a single moving camera. The paper first proposes to construct the cost volume using SSIM as a photometric measure. To deal with dynamic objects commonly found in outdoor scenes, a novel MaskModule is proposed, which can predict the moving object mask based on the input cost quantity. Using the predicted mask, using the proposed DepthModule is able to estimate the accurate depth of static and dynamic objects. In addition, we propose a novel multi-stage training scheme as well as a semi-supervised loss formula for training depth prediction. Taken together, MonoRec is able to qualitatively and quantitatively outperform the latest MVS and monocular depth prediction methods on KITTI, and performs well on Oxford RobotCar and TUM-Mono. This ability to recover an accurate 3D dense point cloud from a single moving camera will help establish the camera as a pilot sensor for intelligence systems. 



--------------------------------------------------------------------------------

multi-view visualization 

 This paper explores how to display multiple point cloud images in one window in PCL library. 

 The main functions are as follows: 

  ```python  
After clicking on the GitHub Sponsor button above, you will obtain access permissions to my private code repository ( https://github.com/slowlon/my_code_bar ) to view this blog code. By searching the code number of this blog, you can find the code you need, code number is: 2024020309573785721
  ```  
 createViewPort is a function used to create a new viewport. The required four parameters are the minimum and maximum values of the viewport on the X axis, and the minimum and maximum values on the Y axis, with values between 0 and 1. 

 Double view window example 

  ```python  
After clicking on the GitHub Sponsor button above, you will obtain access permissions to my private code repository ( https://github.com/slowlon/my_code_bar ) to view this blog code. By searching the code number of this blog, you can find the code you need, code number is: 2024020309573785721
  ```  
 Analysis: The coordinate origin is in the upper left corner. The v1 viewport (xmin = 0, ymin = 0, xmax = 0.5, ymax = 1.0) means that its x is between 0-0.5, which is half of the window. 

 Three windows. 

  ```python  
After clicking on the GitHub Sponsor button above, you will obtain access permissions to my private code repository ( https://github.com/slowlon/my_code_bar ) to view this blog code. By searching the code number of this blog, you can find the code you need, code number is: 2024020309573785721
  ```  
 You can push out the 4 viewports of the Laitian grid, as follows viewer- > creatViewPort (0.0, 0.0, 0.5, 0.5, v1); viewer- > creatViewPort (0.5, 0.0, 1.0, 0.5, v2); viewer- > creatViewPort (0.0, 0.5, 0.5, 1.0, v3); viewer- > creatViewPort (0.5, 0.5, 1.0, 1.0, v4); 

 At present, the WeChat exchange group is growing, due to the large number of people, there are currently two groups, in order to encourage everyone to share, we hope that everyone can actively share while learning, and send your questions or small summary submissions to the group owner mailbox main mailbox dianyunpcl@163.com. 

 ![avatar]( 20200121193538953.jpeg) 

 If the above content is wrong or needs to be added, please leave a message! At the same time, everyone is welcome to pay attention to the WeChat official account, actively share submissions, or join the 3D visual WeChat group or QQ exchange group. Original is not easy, please contact the group owner for reprinting and indicate the source  



--------------------------------------------------------------------------------

#  Statistical outlier removal 

 Principle: Traverse the point cloud, calculate the average distance between each point and its k nearest neighbor points, and then calculate the mean and standard deviation of all the average distances, then the distance threshold can be expressed as, α is a proportional coefficient, traverse the point cloud again, and the k neighbors The point whose average distance is greater than the point is marked as an outlier. Method: remove_statistical_outlier (nb_neighbors, std_ratio) Parameters: nb_neighbors, the n points closest to the point that participate in the calculation when judging a point. std_ratio: The ratio of the standard deviation of all point distances, the larger the value, the more outliers. Return value: Returns a tuple, the previous value is the point cloud object that removes outliers, and the latter value is the point cloud number after removing outliers. 

 ![avatar]( 5d8ad0f8a3cb4f91af010f6891bbbdc9.gif) 

 Effect display:  

#  Radius outlier removal 

 Principle: Count the number of points in the neighborhood of a sphere whose radius is radius from a point. If it is less than nb_points, mark the point as an outlier. Method: remove_radius_outlier (nb_points, radius) Parameters: nb_points: Determine whether a point is around the threshold value of the number of outliers. Radius: A point is the radius of the neighborhood of the sphere in. Return value: Return a tuple, the previous value is the point cloud object that removes outliers, and the latter value is the point cloud number after removing outliers. Effect display: 

 ![avatar]( bc8d8008a361425ea343259ec4f233c0.gif) 

#  III. Code 

  ```python  
After clicking on the GitHub Sponsor button above, you will obtain access permissions to my private code repository ( https://github.com/slowlon/my_code_bar ) to view this blog code. By searching the code number of this blog, you can find the code you need, code number is: 2024020309573718881
  ```  
#  IV. Reference 

 http://www.open3d.org/docs/latest/tutorial/Advanced/pointcloud_outlier_removal.html 



--------------------------------------------------------------------------------

![avatar]( 88947bde55b545f7ab3d45de175b3721.png) 

  Objective: To give different colors to the category labels of gt and dt boxes for easy distinction. 

  ```python  
After clicking on the GitHub Sponsor button above, you will obtain access permissions to my private code repository ( https://github.com/slowlon/my_code_bar ) to view this blog code. By searching the code number of this blog, you can find the code you need, code number is: 2024020309573723966
  ```  


--------------------------------------------------------------------------------

#  Demo display 

 ![avatar]( 79ca68d49453423baad7d9d86a073052.gif) 

  Algorithm 1:  

 ![avatar]( d07133d5f45d4373a374f102a395e152.gif) 

 Algorithm 2:  

 ![avatar]( 31ac99dae8d745e1b49c16b99ac4ab1a.gif) 

 Algorithm 3:  

#  Project address 

 GitHub: Project address, paper link: paper address 

#  Reproduction process 

 ![avatar]( 8c568449a9df425f92dd7810f45e3cf9.png) 

 1. Source code download 2. Unzip, and then copy all the .h and .cpp files in the src folder to the new vs project. 3. Create a new vs project, and add the header files and source files just copied to the project, as shown in the figure below. 3. Configure the relevant environment, as shown in the figure below. Just add the properties file of opencv. Add the pcl related files here to facilitate subsequent operation changes.   

 4. There is a problem 1) The file "opencv/cv.h" cannot be found, which is due to version reasons. Just comment the opencv/cv.h file and replace it with the following header file. 

  ```python  
After clicking on the GitHub Sponsor button above, you will obtain access permissions to my private code repository ( https://github.com/slowlon/my_code_bar ) to view this blog code. By searching the code number of this blog, you can find the code you need, code number is: 2024020309573761589
  ```  
 2) c4996: "fopen...", change as follows: 

 ![avatar]( 61f7434343c940dfbe2071febde1d66c.png) 

 ![avatar]( 047c20f6b76c4e27aa59662faf4ae825.png) 

  At this point, rebuild it and complete the testing of the relevant project. 

#  follow-up 

 This method is still excellent as a whole and can be used directly in some industrial applications. It will continue to be integrated into QT + PCL in the future to learn and progress together. 



--------------------------------------------------------------------------------



--------------------------------------------------------------------------------

#  PCD file format 

 Although there are many file types for 3D point cloud data, the existing file structure does not support some extensions in the processing of the n-dimensional point type mechanism introduced by the PCL library due to its composition. So PCL officials chose the PCD file format. 

 Other point cloud formats: 

>  PLY: A polygon file format STL: Modeling file format, mainly used in CAD and CAM fields OBJ: Geometrically defined file format X3D: is an XML-based file format that conforms to ISO standards and is used to represent 3D computer graphics data 

#   PCD version 

 Before the release of Point Cloud Library (PCL) 1.0, the PCD file format had different revision numbers, and the official PCD file format released in PCL was version 0.7 (PCD_V7). 

#  file header format 

 Each PCD file contains a file header that identifies and declares certain characteristics of the point cloud data stored in the file. The PCD file header must be encoded in ASCII code. 

 PCD file example: 

  ```python  
After clicking on the GitHub Sponsor button above, you will obtain access permissions to my private code repository ( https://github.com/slowlon/my_code_bar ) to view this blog code. By searching the code number of this blog, you can find the code you need, code number is: 2024020309573786453
  ```  
>  #. PCD v0.7 - Point Cloud Data file format//Comment VERSION 0.7//PCD file version 

>  FIELDS x y z r g b intensity timestamp//What dimensions does each point contain, xyz represents XYZ three-dimensional coordinates, rgb represents color (which can be expressed separately or as a floating point number), intensity represents laser reflection intensity, timestamp represents timestamp, normal_x, normal_y, normal_z represent plane normal three-dimensional coordinates, j1, j2, j3 represent invariant moments. 

>  SIZE 4 4 4 1 1 1 1 8//Byte size of data occupied by each dimension 

>  TYPE F F F U U U F//The data type of each dimension, I represents the signed type int8 (char), int16 (short), int32 (int), U represents the unsigned type uint8 (unsigned char), uint16 (unsigned short), uint32 (unsigned int), F represents the floating point type 

>  COUNT 1 1 1 1 1 1 1 1//How many elements per dimension (if no COUNT attribute is provided, the default value is 1) 

>  WIDTH 32//Use the number of points to represent the width of the point cloud dataset. There are two meanings: 1. The number of points in the point cloud of the unordered dataset 2. The width of the ordered point cloud dataset (the number of points in a row), the ordered point cloud dataset, the point cloud is a structure similar to a picture or matrix, divided into rows and columns. This data usually comes from stereo cameras, Time Of Flight cameras, which use infrared rays or light pulses to estimate the time lag of light from emission to detection to measure the distance), knowing the adjacent relationship of points, making the algorithm calculation more efficient. 

>  HEIGHT 2172//Use the number of points in the point cloud dataset to represent the height of the point cloud dataset. Height has the following two meanings: 1. Ordered point cloud dataset, number of rows 2. Disordered point cloud dataset, height 1 (can be used to determine whether a dataset is ordered or disordered) 

>  VIEWPOINT 0 0 0 1 0 0 0//Specifies the collection viewpoint of the points in the data set. Can be used for subsequent possible coordinate transformations, or to find plane normal coordinates. The format is translation (tx ty tz) + quaternion (qw qx qy qz), the default is 0 0 0 1 0 0 0 0. 

>  POINTS 69504//Total number of points in the point cloud (redundant field) 

>  DATA binary_compressed//storage type of point cloud data, version 0.7 supports two storage methods: ascii and binary. 

>  The order in the file format header cannot be changed, that is, it must be the following order: VERSION, FIELDS, SIZE, TYPE, COUNT, WIDTH, HEIGHT, VIEWPOINT, POINTS, DATA 

  Reference article: 

 66. [PCL] PCD file format _Mars Loo's Blog-CSDN Blog _pcd file 

 PCD (Point Cloud Data) file format 



--------------------------------------------------------------------------------

#  foreword 

#  Point cloud rotation translation transformation 

  ```python  
After clicking on the GitHub Sponsor button above, you will obtain access permissions to my private code repository ( https://github.com/slowlon/my_code_bar ) to view this blog code. By searching the code number of this blog, you can find the code you need, code number is: 2024020309573749167
  ```  
 The core is in pcl :: trans form Point Cloud (* cloud, * cloud_trans, trans); 

#  Second, mesh rotation translation transformation 

  ```python  
After clicking on the GitHub Sponsor button above, you will obtain access permissions to my private code repository ( https://github.com/slowlon/my_code_bar ) to view this blog code. By searching the code number of this blog, you can find the code you need, code number is: 2024020309573749167
  ```  
 ![avatar]( 5120e778861243abafea890dcd71fc31.png) 

 ![avatar]( bd9bb859e6954a0b82ad429869b2dc54.png) 

  ```python  
After clicking on the GitHub Sponsor button above, you will obtain access permissions to my private code repository ( https://github.com/slowlon/my_code_bar ) to view this blog code. By searching the code number of this blog, you can find the code you need, code number is: 2024020309573749167
  ```  
  ```python  
After clicking on the GitHub Sponsor button above, you will obtain access permissions to my private code repository ( https://github.com/slowlon/my_code_bar ) to view this blog code. By searching the code number of this blog, you can find the code you need, code number is: 2024020309573749167
  ```  
#  Effect display 

 ![avatar]( 0832948931104a8892543e7cd6f2cccc.png) 

#  IV. Overall code 

  ```python  
After clicking on the GitHub Sponsor button above, you will obtain access permissions to my private code repository ( https://github.com/slowlon/my_code_bar ) to view this blog code. By searching the code number of this blog, you can find the code you need, code number is: 2024020309573749167
  ```  
  OVER!!! 



--------------------------------------------------------------------------------

In PCL's point cloud library, a large number of dynamic memory programming methods are used, such as: 

  ```python  
After clicking on the GitHub Sponsor button above, you will obtain access permissions to my private code repository ( https://github.com/slowlon/my_code_bar ) to view this blog code. By searching the code number of this blog, you can find the code you need, code number is: 2024020309573735826
  ```  
 The first line is to declare a dynamic memory for storing point cloud data, so here I make a note about the dynamic memory problem in C++. In addition to static memory and stack memory, each program also has a memory pool. This part of the memory is called free space or stack. The program uses the heap to store dynamically allocated objects. The lifetime of these dynamic objects is controlled by the program. It can be understood that when dynamic memory is no longer used, the code must explicitly destroy them. The management of dynamic memory in C++ is done by a pair of operators: new allocates space for the object in dynamic memory and returns a pointer to the object, which can be selected to initialize the object; delete destroys the dynamic object pointer and frees the associated memory, so the management of dynamic memory is prone to problems, because it is very difficult to ensure that the memory is freed at the correct time. If we forget to free the memory, in this case it will lead to memory leakage. If the pointer references the memory, it is easy to refer to the illegal memory pointer. So in short, the operator New allocates memory, delete frees New's allocated memory, and uses New's method of dynamically allocating and initializing objects. Int * pi = new int means that pi points to a dynamically allocated uninitialized unnamed object. The expression indicates that an int-type object is constructed in free space and returns a pointer to the object. Of course, you can also use a list to initialize int * pi = new int (1024); the value of the object pointed to by pi is 1024, such as vector * ps = new string (10, '9'); it means that * ps is 999999999, which can also be used to initialize dynamically allocated objects. Just follow the type name with a pair of empty parentheses. * ps1 = new string (); value initialized to empty string  

 C++ new standard, two types of smart pointers are provided to manage dynamic objects. Smart pointers behave like regular pointers. The important difference is that they can automatically release the pointed object, shared_ptr allow multiple pointers to point to the same object, and unique_ptr to monopolize the pointed object. The difference between the two methods is that the underlying pointers are managed differently. Similar to vector smart pointers, there are templates, which give the data type in angle brackets, followed by the name of the defined smart pointer: 

  ```python  
After clicking on the GitHub Sponsor button above, you will obtain access permissions to my private code repository ( https://github.com/slowlon/my_code_bar ) to view this blog code. By searching the code number of this blog, you can find the code you need, code number is: 2024020309573735826
  ```  
 When dereferencing a smart pointer, returning the object it points to, one of the safest ways to allocate and use memory is to call a standard library function called make_shared, which allocates an object in dynamic memory and initializes it, returning the shared_Ptr to this object Like the smart pointer, make_shared is also defined in the memory of the C++ library. When using make_shared, you must specify the type of object you want to create, giving a simple instance: 

  ```python  
After clicking on the GitHub Sponsor button above, you will obtain access permissions to my private code repository ( https://github.com/slowlon/my_code_bar ) to view this blog code. By searching the code number of this blog, you can find the code you need, code number is: 2024020309573735826
  ```  
 When the copy and assignment operation, each shared_ptr will record how many other shared_ptr point to the same object, so you can think of each shared_ptr has an associated counter, called reference count, so whenever you copy a shared_ptr counter will be incremented, and when we give shared_ptr a new value or shared_ptr destroyed, the counter will be decremented, once a shared_ptr counter becomes 0, it will automatically release the object it manages, so when the last shared_ptr pointed to the object is destroyed, shared_Ptr class will automatically destroy the object, through another special member function called destructor (destructor) to complete the destruction work, so the destructor is used to release the resources allocated by the object, shared_ptr destructor Decreases the reference count of the object it points to. If the reference becomes 0, shared_pre destructor Hasselblad destroys the object and frees the memory it occupies. Here is an example of the definition in PCL 

  ```python  
After clicking on the GitHub Sponsor button above, you will obtain access permissions to my private code repository ( https://github.com/slowlon/my_code_bar ) to view this blog code. By searching the code number of this blog, you can find the code you need, code number is: 2024020309573735826
  ```  
 This example is the content of implementing plane extraction, which involves the data definition of point cloud data transposition PCL in ROS, which involves the use of cloud.makeShared (), so we can compare it to the definition: 

  ```python  
After clicking on the GitHub Sponsor button above, you will obtain access permissions to my private code repository ( https://github.com/slowlon/my_code_bar ) to view this blog code. By searching the code number of this blog, you can find the code you need, code number is: 2024020309573735826
  ```  
 ![avatar]( 201805092111147) 

 Here read and compare seems to be able to understand what, this is also a lot of people confused, next I will try to make more time to write blog debugging code, may write some about the conversion between various formats of 3D data, as well as the method of CAD model sampling check point cloud, so please pay more attention to Weixin official account, share with us, if you have any questions, please point out that the record of taking notes in this way, let me understand the article more deeply, and welcome everyone to pay attention to WeChat official account or join the 3D visual WeChat group to communicate and share the submission email dianyunpcl@163.com 



--------------------------------------------------------------------------------

Here I will share some of the pits I encountered using the PCL library, as well as summary skills. Of course, you need to share more and forward the official account articles or knowledge planet articles to Moments. 

 pcl_common mainly contains common data structures and methods commonly used in PCL libraries, such as PointCloud classes and many types used to represent points, surfaces, normal vectors, feature descriptions, etc., functions for calculating distance, mean and covariance, angle conversion and geometric changes. 

 For various points, the data structures of the types of features will not be exemplified here, which depends on the actual situation. 

 Here mainly introduce the basic common function functions, these functions in fact C++ can also be implemented by themselves, in the PCL provides more overload interface, easy to use. 

 Header files in common modules 

 Angles .h defines the angle calculation function of the standard C interface 

 Centriod.h defines the estimation of the center point and the calculation of the covariance matrix 

 Como.h standard C and C++ classes, which are the parent classes of other common functions 

 Distance.h defines the standard C interface for calculating distances 

 file_io defines some functions that help write or read files. 

 Random.h defines some functions generated by random point clouds 

 Geometry.h functions that define some basic geometric functions 

 Intersection.h defines the function where lines intersect 

 Norm.h defines the standard C method for calculating the regularization of matrices 

 Time.h defines a function for calculating time 

 Point_types defines the types of data structures for all PCL-implemented point clouds 

 Basic functions in common modules 

 pcl :: rad2deg (flat alpha) 

 From radians to angles 

 pcl :: deg2rad (float aipha) 

 From Angle to Radian 

 pcl::normAngle(float alpha) 

 The regularization angle is between (-PI, PI) 

 pcl::compute3DCentroid (const pcl::PointCloud< PointT > &cloud, Eigen::Matrix< Scalar, 4, 1 > &centroid) 

 Computes the 3D center point of a given group of points and returns a 3D vector 

 pcl::computeCovarianceMatrix (const pcl::PointCloud< PointT > &cloud, const Eigen::Matrix< Scalar, 4, 1 > &centroid, Eigen::Matrix< Scalar, 3, 3 > &covariance_matrix) 

 Compute the covariance matrix for a given three-dimensional point cloud. 

 pcl::computeMeanAndCovarianceMatrix (const pcl::PointCloud< PointT > &cloud, Eigen::Matrix< Scalar, 3, 3 > &covariance_matrix, Eigen::Matrix< Scalar, 4, 1 > &centroid 

 Compute the regularized 3 * 3 covariance matrix and the center point of the given point cloud data 

 pcl::demeanPointCloud (const pcl::PointCloud< PointT > &cloud_in, const Eigen::Matrix< Scalar, 4, 1 > &centroid, pcl::PointCloud< PointT > &cloud_out) 

 pcl:: computeNDCentroid (const pcl:: PointCloud < PointT > & cloud, Own:: Matrix < Scalar, Own:: Dynamic, 1 > & centroid) 

 The index of a set of points is used to estimate the nD center. 

 pcl:: getAngle3D (const Own:: Vector4f & v1, const Own:: Vector4f & v2, const bool in _ degree = false) 

 Calculate the angle between two vectors 

 pcl::getMeanStd (const std::vector< float > &values, double &mean, double &stddev) 

 Simultaneously calculate the mean and standard deviation of the given point cloud data 

 pcl::getPointsInBox (const pcl::PointCloud< PointT > &cloud, Eigen::Vector4f &min_pt, Eigen::Vector4f &max_pt, std::vector< int > &indices) 

 Gets a set of points located in the box given a boundary 

 pcl::getMaxDistance (const pcl::PointCloud< PointT > &cloud, const Eigen::Vector4f &pivot_pt, Eigen::Vector4f &max_pt) 

 The value of the maximum distance between points in a given point cloud data 

 pcl::getMinMax3D (const pcl::PointCloud< PointT > &cloud, PointT &min_pt, PointT &max_pt) 

 Gets the maximum and minimum values on the XYZ axis in a given point cloud 

 pcl::getCircumcircleRadius (const PointT &pa, const PointT &pb, const PointT &pc) 

 Calculate the circumcircle radius of a triangle composed of three points pa, pb and pc. 

 pcl::getMinMax (const PointT &histogram, int len, float &min_p, float &max_p) 

 Gets the minimum and maximum values on the point histogram. 

 pcl::calculatePolygonArea (const pcl::PointCloud< PointT > &polygon) 

 Calculate the area of a polygon from the point cloud of a given polygon 

 pcl::copyPoint (const PointInT &point_in, PointOutT &point_out) 

 Assign field data from Point_in to Point_out 

 pcl:: lineToLineSegment (const Own:: VectorXf & line _ a, const Own:: VectorXf & line _ b, Own:: Vector4f & pt1 _ seg, Own:: Vector4f & pt2 _ seg) 

 Get the shortest 3D line segment between two 3D lines 

 pcl::sqrPointToLineDistance (const Eigen::Vector4f &pt, const Eigen::Vector4f &line_pt, const Eigen::Vector4f &line_dir) 

 Get the square distance from point to line (represented by points and directions) 

 pcl::getMaxSegment (const pcl::PointCloud< PointT > &cloud, PointT &pmin, PointT &pmax) 

 Gets the maximum segment in a given set of points, and returns the minimum and maximum points. 

 pcl:: eigen22 (const Matrix & mat, typename Matrix:: Scalar & eigenvalue, Vector & eigenvector) 

 Determine the minimum eigenvalue and its corresponding eigenvector 

 pcl::computeCorrespondingEigenVector (const Matrix &mat, const typename Matrix::Scalar &eigenvalue, Vector &eigenvector) 

 Determining the Eigenvector Corresponding to the Given Eigenvalue of Symmetric Positive Semidefinite Input Matrix 

 pcl:: eigen33 (const Matrix & mat, typename Matrix:: Scalar & eigenvalue, Vector & eigenvector) 

 Determining Eigenvector and Eigenvalue of Minimum Eigenvalue of Symmetric Positive Semidefinite Input Matrix 

 pcl::invert2x2 (const Matrix &matrix, Matrix &inverse) 

 Compute the inverse of a 2x2 matrix. 

 pcl::invert3x3SymMatrix (const Matrix &matrix, Matrix &inverse) 

 Compute the inverse of a 3x3 symmetric matrix. 

 pcl::determinant3x3Matrix (const Matrix &matrix) 

 Calculating the determinant of a 3x3 matrix 

 pcl::getTransFromUnitVectorsZY (const Eigen::Vector3f &z_axis, const Eigen::Vector3f &y_direction, Eigen::Affine3f &transformation) 

 To get a unique 3D rotation, rotate the Z axis to (0, 0, 1) and the Y axis to (0, 1, 0) and the two axes are orthogonal. 

 pcl::getTransformationFromTwoUnitVectorsAndOrigin (const Eigen::Vector3f &y_direction, const Eigen::Vector3f &z_axis, const Eigen::Vector3f &origin, Eigen::Affine3f &transformation) 

 Get the transformation that converts origin to (0, 0, 0) and rotates the Z axis into (0, 0, 1) and the Y direction (0, 1, 0). 

 pcl::getEulerAngles (const Eigen::Transform< Scalar, 3, Eigen::Affine > &t, Scalar &roll, Scalar &pitch, Scalar &yaw) 

 Extracting the Euler angle from a given transformation matrix 

 pcl::getTranslationAndEulerAngles (const Eigen::Transform< Scalar, 3, Eigen::Affine > &t, Scalar &x, Scalar &y, Scalar &z, Scalar &roll, Scalar &pitch, Scalar &yaw) 

 In the given transformation, extract XYZ and Euler angles 

 pcl::getTransformation (float x, float y, float z, float roll, float pitch, float yaw) 

 Create a transformation matrix from a given translation and Euler angle 

 pcl::saveBinary (const Eigen::MatrixBase< Derived > &matrix, std::ostream &file) 

 Save or write the matrix to an output stream 

 pcl::loadBinary (Eigen::MatrixBase< Derived > const &matrix, std::istream &file) 

 Read matrix from input stream 

 pcl::lineWithLineIntersection (const Eigen::VectorXf &line_a, const Eigen::VectorXf &line_b, Eigen::Vector4f &point, double sqr_eps=1e-4) 

 Gets two 3D lines in space as the intersection of 3D points. 

 pcl::getFieldIndex (const pcl::PCLPointCloud2 &cloud, const std::string &field_name) 

 Get the index of the specified field (i.e. dimension/channel) 

 pcl::getFieldsList (const pcl::PointCloud< PointT > &cloud) 

 Gets a list of all available fields in the cloud at a given point 

 pcl::getFieldSize (const int datatype) 

 Gets the size (in bytes) of a specific field data type. 

 pcl::concatenatePointCloud (const pcl::PCLPointCloud2 &cloud1, const pcl::PCLPointCloud2 &cloud2, pcl::PCLPointCloud2 &cloud_out) 

 Connect PCL :: PCL Point Cloud 2 type point cloud field 

 If you are interested in this article, please click "Original Reading" to get the QR code of Knowledge Planet. Be sure to join the free Knowledge Planet according to the remarks of "Name + School/Company + Research Direction", download the pdf document for free, and communicate with more friends who love to share! 

 ![avatar]( 20200705154029688.PNG) 



--------------------------------------------------------------------------------

 I have just started to come into contact with PCL, and I know very little, so there are always various problems. Every time I encounter a problem, I have to find various materials, which is very time-consuming. Therefore, today I will share the common problems I have encountered with you, and explain the steps in as much detail as possible, so that friends with poor foundation like me can enter the learning of PCL point cloud library as soon as possible, hoping to make progress with you. 

 Runtime environment: PCL-1.8.0 -AllInOne-msvc2013-win64, is 64-bit, VS2013 English version. 

 Question 1: How to get the PCD file. A friend asked me how to get the pcd file before. I know this is a very basic question, but newbies often ask this question, including when I just started learning by myself. Usually there are two ways. 

 Way1: One is to convert through cloudcompare software, which can be downloaded from its official website, which is more direct for beginners. 

 Way2: Write the code yourself. 

 Question 2: The error message is 1. IntelliSense: cannot open source file "pcl/io/pcd_io" c:\ visual, etc. As shown in the figure below, check if your compilation platform has been changed to 64-bit. 

 ![avatar]( aHR0cHM6Ly9tbWJpei5xbG9nby5jbi9tbWJpel9wbmcvOXFkYmV3OGlhWkxnUjBqc3dpYU9qS29mWXk0V0p3NnJpY29lUkgwSWJRc1cwODBSbG5TdmVwYm00ZXI2ZnU5MWt3NGZLQXVtaWNoeHVlZFVmVDNjRTBOQkxRLzA_d3hfZm10PXBuZw) 

 Solution: 

 Step 1: 

 ![avatar]( aHR0cHM6Ly9tbWJpei5xbG9nby5jbi9tbWJpel9wbmcvOXFkYmV3OGlhWkxnUjBqc3dpYU9qS29mWXk0V0p3NnJpY29DVVh0YmJQWm1hU0NTcTFoQnI2N1RhMmhLR2hXemljQ2hKQmVuTW8ySDExN3p5aFJlMkZrdnN3LzA_d3hfZm10PXBuZw) 

 Step 2: 

 ![avatar]( aHR0cHM6Ly9tbWJpei5xbG9nby5jbi9tbWJpel9wbmcvOXFkYmV3OGlhWkxnUjBqc3dpYU9qS29mWXk0V0p3NnJpY29pYk93aG1aWEFhMWdySUpreEtpYlo1SUhpYkQ3Z3FRTmZnbEJjUG1oeUVqUGNEREE1cHZ1ek01YWcvMD93eF9mbXQ9cG5n) 

 Problem 3: Error report similar problems such as 

 Error         3       error C4996: 'std::_Uninitialized_copy0':Function call with parameters that may be unsafe - this call relies on thecaller to check that the passed values are correct. To disable this warning,use -D_SCL_SECURE_NO_WARNINGS.See documentation on how to use Visual C++ 'Checked Iterators'       C:\Program Files (x86)\Microsoft VisualStudio 12.0\VC\include\xmemory       348 

 ![avatar]( aHR0cHM6Ly9tbWJpei5xbG9nby5jbi9tbWJpel9wbmcvOXFkYmV3OGlhWkxnUjBqc3dpYU9qS29mWXk0V0p3NnJpY29qY2g5TGdZOTJqaWJFSjlyNkRXTndIcmdNN29YanlsaWM1cjlsV0NJcWdJUUtRc2tKanBac1AzZy8wP3d4X2ZtdD1wbmc) 

 Solution: 

 Step 1: Open the property sheet. 

 ![avatar]( aHR0cHM6Ly9tbWJpei5xbG9nby5jbi9tbWJpel9wbmcvOXFkYmV3OGlhWkxnUjBqc3dpYU9qS29mWXk0V0p3NnJpY29YYVM4d3NlV2U3OTNxd2ZzNVZYUlJTRnBYaWFpYmRpYVlBOUVGaWFJdmczd2tDUmtwT2lhcVdKeFYzQS8wP3d4X2ZtdD1wbmc) 

 Step 2: Add _SCL_SECURE_NO_WARNINGS to the preprocessor definition as shown 

 ![avatar]( aHR0cHM6Ly9tbWJpei5xbG9nby5jbi9tbWJpel9wbmcvOXFkYmV3OGlhWkxnUjBqc3dpYU9qS29mWXk0V0p3NnJpY29MZXVPcERQd292bEFKd3JYdzhKRjBNUEtGNUhVc3RsQ0hQenlpYXR4UmcxTWxRWTFBTFlRS3ZnLzA_d3hfZm10PXBuZw) 

 Note: If the error message above is C4996: 'fopen ’*******_ CRT _ SECURE _ NO _ WARNINGS ********, follow the steps above to add _CRT_SECURE_NO_WARNINGS to the preprocessor definition. 

 Problem 4: I encountered the following error message when compiling 

 error C4996: 'pcl::SAC_SAMPLE_SIZE': Thismap is deprecated and is kept only to prevent breaking existing user code. Startingfrom PCL 1.8.0 model sample size is a protected member of theSampleConsensusModel class. 

 This is a problem with the program life cycle check. 

 Solution: 

 Open Project Properties Page > C/C++ > General > SDL Check (set to No). 

 Problem 5: I encountered the following error message when compiling 

 error C1128: number of sections exceededobject file format limit : compile with /bigobj 

 Solution: 

 Right-click the project, properties - > Configuration Properties - > C/C++ - > Command Line - > Additional options, then add /bigobj properties, OK, and then recompile. 

 I am very grateful to this classmate for sharing and summarizing like this. I am very touched. My original intention is to hope that everyone can share in this way and provide some suggestions for beginners. Learn from each other and progress. 

 So it is recommended that after studying for a period of time, you can write a little summary and share it with everyone 

 Interested parties scan the QR code to follow the WeChat official account, and the background can directly private message me 

 ![avatar]( aHR0cHM6Ly9tbWJpei5xbG9nby5jbi9tbWJpel9wbmcvOXFkYmV3OGlhWkxqMXNFaWJRTHZRVmdic0Z1T3ZqRk9mT0kzSlR4ODRldXh2N2FaaWNPZ3B1aWFheWhqUWljMzNwNEh0QWY5amZGSUk1Um1pYWUyZmNmZFdNN1EvMD93eF9mbXQ9cG5n) 



--------------------------------------------------------------------------------

pcl_common library contains common data structures and methods used by most PCL libraries. The core data structures include the PointCloud class and many point types used to represent points, surface normals, RGB color values, feature descriptors, etc. It also contains many used to calculate distance/norm, mean and covariance, angle conversion, geometric transformation, etc. This module is not dependent on other modules, so it can be compiled successfully separately. Compiled separately, you can use the data structure to develop it yourself. Of course, if you want to extract it separately, you need to modify cmakeLists by yourself when compiling. I will not repeat it here. Then we will explain the role of each function in order. If necessary, I will explain its theory and combine code practice. 

 PCL_common classes: 

 (1) class pcl :: BivariatePolynomialT < real > This represents a binary polynomial and provides some functional interfaces to it. 

 (2) class pcl :: Centroid Point < PointT > A generic class that computes the centroid to the input point cloud. Here we use the "center of gravity" to represent not only the average of 3D point coordinates, but also the average of values in other data fields. The generic computeNDroid Centroid () function also implements this functionality, but it does so in an "unintelligent" way, that is, it simply averages values regardless of the semantics of the data within the field. In some cases (e.g., for x, y, z, intensity fields), this behavior is reasonable, but in other cases (e.g., rgb, rgba, rgbl (labeled with labels)), this does not lead to meaningful results. 

 This class is able to calculate the centroid in an "intelligent" way, taking into account the meaning of the data within the field. The following fields are currently supported: 

  ```python  
After clicking on the GitHub Sponsor button above, you will obtain access permissions to my private code repository ( https://github.com/slowlon/my_code_bar ) to view this blog code. By searching the code number of this blog, you can find the code you need, code number is: 2024020309573770504
  ```  
 ![avatar]( 20191220191454118.png) 

 (3) struct pcl :: NdConcatenateFunctor < PointInT, PointOutT > point cloud point set addition auxiliary function, here to specifically declare the point cloud library point cloud addition in two ways: For example: cloud_c = cloud_a; cloud_c += cloud_b;//cloud_a and cloud_b connection together to create cloud_c output, the output is as follows: field addition will use the auxiliary function, then the output is as follows: (4) class pcl :: Feature Histogram used to calculate some floating-point number mean and variance histogram type. 

  ```python  
After clicking on the GitHub Sponsor button above, you will obtain access permissions to my private code repository ( https://github.com/slowlon/my_code_bar ) to view this blog code. By searching the code number of this blog, you can find the code you need, code number is: 2024020309573770504
  ```  
 (5) class pcl :: Gaussian Kernel The Gaussian Kernel class integrates all methods for computing images using Gaussian kernels, convolution, smoothing, and layer. 

  ```python  
After clicking on the GitHub Sponsor button above, you will obtain access permissions to my private code repository ( https://github.com/slowlon/my_code_bar ) to view this blog code. By searching the code number of this blog, you can find the code you need, code number is: 2024020309573770504
  ```  
 (6) class pcl :: PCA < PointT > Principal Component Analysis (PCA) class. The principal components are extracted by performing a singular value decomposition of the covariance matrix of the center points of the input point cluster. The available data after the PCA calculation are the mean of the input data, the eigenvalues (in descending order), and the corresponding eigenvectors. Other methods allow projection in the feature space, reconstruction from the feature space, and updating the feature space with new data (according to Matej Artec, Matjaz Jogan and Ales Leonardis: "Incremental PCA for On-line Visual Learning and Recognition"). 

  ```python  
After clicking on the GitHub Sponsor button above, you will obtain access permissions to my private code repository ( https://github.com/slowlon/my_code_bar ) to view this blog code. By searching the code number of this blog, you can find the code you need, code number is: 2024020309573770504
  ```  
 Class PCL :: PiecewiseLinearFunction provides the ability to return the value of a valid piecewise linear function. 

  ```python  
After clicking on the GitHub Sponsor button above, you will obtain access permissions to my private code repository ( https://github.com/slowlon/my_code_bar ) to view this blog code. By searching the code number of this blog, you can find the code you need, code number is: 2024020309573770504
  ```  
 (8) class pcl :: PolynomialCalculationsT < real > provides some functionality for polynomials, such as finding roots or approximating binary polynomials. 

  ```python  
After clicking on the GitHub Sponsor button above, you will obtain access permissions to my private code repository ( https://github.com/slowlon/my_code_bar ) to view this blog code. By searching the code number of this blog, you can find the code you need, code number is: 2024020309573770504
  ```  
 (9) class pcl :: Poses From Matches Compute three-dimensional space transformations between points based on their corresponding relationships 

 (10) class PCL :: StopWatch a stopwatch for timing. 

  ```python  
After clicking on the GitHub Sponsor button above, you will obtain access permissions to my private code repository ( https://github.com/slowlon/my_code_bar ) to view this blog code. By searching the code number of this blog, you can find the code you need, code number is: 2024020309573770504
  ```  
 (11) class pcl :: Scope Time measures the time in scope. To use this class, for example to measure the time spent in a function, simply create an instance at the beginning of the function. example 

  ```python  
After clicking on the GitHub Sponsor button above, you will obtain access permissions to my private code repository ( https://github.com/slowlon/my_code_bar ) to view this blog code. By searching the code number of this blog, you can find the code you need, code number is: 2024020309573770504
  ```  
 (12) class pcl :: Event Frequency An auxiliary class to measure the frequency of an event. 

 (13) class pcl :: Time Trigger Timer class that calls the callback function regularly. 

 Class PCL :: TransformationFromCorrespondences compute transformations based on corresponding 3D points. 

  ```python  
After clicking on the GitHub Sponsor button above, you will obtain access permissions to my private code repository ( https://github.com/slowlon/my_code_bar ) to view this blog code. By searching the code number of this blog, you can find the code you need, code number is: 2024020309573770504
  ```  
 (15) class pcl :: Vector Average < real, dimension > Class for computing the weighted average and covariance matrix of a set of vectors for a given weight 

  ```python  
After clicking on the GitHub Sponsor button above, you will obtain access permissions to my private code repository ( https://github.com/slowlon/my_code_bar ) to view this blog code. By searching the code number of this blog, you can find the code you need, code number is: 2024020309573770504
  ```  
 (16) struct pcl :: Correspondence Represents a match between two entities (e.g., points, descriptors, etc.). Represented by the distance between the source point cloud and the target point cloud. 

  ```python  
After clicking on the GitHub Sponsor button above, you will obtain access permissions to my private code repository ( https://github.com/slowlon/my_code_bar ) to view this blog code. By searching the code number of this blog, you can find the code you need, code number is: 2024020309573770504
  ```  
 (17) struct pcl :: PointCorrespondence3D represents a (possible) correspondence between two 3D points in two different coordinate systems (e.g. from feature matching) (18) struct pcl :: PointCorrespondence6D represents a (possible) correspondence between two points (e.g. from feature matching) and is a 6DOF transformation. 

 ![avatar]( 20191220192014217.png) 

 These three classes are inherited relationships. (19) The following is the form of a point cloud that has been defined in the point cloud library 

 Struct pcl :: Point XYZ Point set structure type for Euclidean xyz coordinates struct pcl :: Intensity Point set structure type for grey release intensity of a single channel image struct pcl :: Intensity 8u Point set structure type for grey release intensity of a single channel image struct pcl :: Intensity 32 u Point set structure type for grey release intensity of a single channel image struct pcl :: _PointXYZI Point set structure and intensity values for Euclidean XYZ coordinates struct pcl :: Point XYZ RGBA Point set structure type for Euclidean XYZ coordinates and RGBA colors struct pcl :: Point XYZ RGB Point set structure type for Euclidean XYZ coordinates and RGB colors struct pcl :: Point XY Two-dimensional point set structure type for Euclidean xy coordinates Struct pcl :: Point UV represents a 2D point set structure type for pixel image coordinates struct pcl :: Interest Point represents a point set structure type with Euclidean xyz coordinates and interest values, give an example about this particular type: 

  ```python  
After clicking on the GitHub Sponsor button above, you will obtain access permissions to my private code repository ( https://github.com/slowlon/my_code_bar ) to view this blog code. By searching the code number of this blog, you can find the code you need, code number is: 2024020309573770504
  ```  
 Struct pcl :: Normal represents normal vector coordinates and curvature estimates for point set structure type struct pcl :: Axis Usage vector coordinates represent point set structure of axes struct pcl :: Point Normal represents point set structure of Euclidean xyz coordinates, together with normal coordinates and surface curvature estimates struct pcl :: Point XYZ RGB Normal represents point set structure of Euclidean xyz coordinates and RGB colors, together with normal coordinates and surface curvature estimators. struct pcl :: Point XYZ IN ormal represents Euclidean xyz coordinates, intensity, together with normal coordinates and surface curvature estimates for point set structure type. Struct pcl :: Point XYZ LN ormal represents Euclidean xyz coordinates, a label, normal coordinates and surface curvature estimation of the point set structure type struct pcl :: Point With Range represents Euclidean XYZ coordinates of the point set structure, and together with the depth information of the floating point number struct pcl :: Point With Viewpoint represents Euclidean xyz coordinates of the point set structure as well as the viewpoint struct pcl :: Moment Invariants represents the point set structure type struct pcl :: Principal Radii RSD represents the minimum and maximum surface radii (in meters) calculated using RSD struct pcl :: Boundary represents whether the point is located at the surface boundary struct pcl :: Principal Curvatures represents the main curvature and its amplitude of the point set structure structure 

 (20) The following is the point set structure of some three-dimensional feature point descriptors, each of which is a paper. We hope that interested friends will join us to analyze and explain the origin of a descriptor and theoretical research. 

 Struct pcl :: PFH Signature 125 Point set structure for point cloud feature histogram (PFH) Type struct pcl :: PFH RGB Signature 250 Point structure for color feature point feature histogram (PFHGB) Struct pcl :: PPF Signature Point set structure for storing Point-to-Feature (PPF) values Struct pcl :: C PPF Signature Point set structure for storing Point-to-Feature (CPPP) values Struct pcl :: PPFRGB Signature Point set structure for storing Point-to-Feature (PPFRGB) values Struct pcl :: Point structure for normal signature based struct pcl for representing 4-By3 feature matrix :: Shape Context 1980 Point structure for shape contexts struct pcl :: Point structure for unique shape contexts struct pcl :: SHOT 352 for OrienTations Point set structure for generic label shapes for histograms (SHOT) struct pcl :: SHOT 1344 A point structure representing a generic signature for an OrienTations histogram (SHOT) - shape + color. struct pcl :: _ReferenceFrame structure representing a local reference system for points struct pcl :: FPFH Signature 33 Point structure representing a quick point feature histogram (FPFH) struct pcl :: VFH Signature 308 Point structure representing a viewpoint feature histogram (VFH) struct pcl :: GRSD Signature 21 Point structure representing a surface descriptor (GRSD) with a global radius. Struct pcl :: BRISK Signature 512 Point structure representing a binary robust invariant scalable keypoint (BRISK). Struct pcl :: ESF Signature 640 represents a point structure for a collection of shape functions (ESF) struct pcl :: GASD Signature 512 represents a globally aligned spatial distribution (GASD) A point structure for a shape descriptor struct pcl :: GASD Signature 984 represents a globally aligned spatial distribution (GASD) A point structure for a shape and color descriptor struct pcl :: GASD Signature 7992 represents a globally aligned spatial distribution (GASD) A point structure for a shape and color descriptor struct pcl :: GFPFH Signature 16 represents a point structure for a GFPFH descriptor with 16 containers. Struct pcl :: N arf 36 represents the point structure of the NARF descriptor struct pcl :: Border Description is used to store the point in the distance image on the boundary between the obstacle and the background struct pcl :: Intensity Gradient represents the point structure of the Xyz point cloud intensity layer struct pcl :: Histogram < N > represents the point structure of the N-D histogram struct pcl :: Point With Scale represents the point structure of the three-dimensional position and scale struct pcl :: Point Surfel surface, i.e. the point structure representing the Euclidean xyz coordinates, together with the normal coordinates, RGBA color, radius, confidence values, and surface curvature estimates. Struct pcl :: Point DEM represents the point structure of the digital elevation map Digital Elevation Map... class pcl :: PCL Base < PointT > PCL base class struct pcl :: Gradient XY represents the point structure and intensity value of the Euclidean XYZ coordinate 

 ![avatar]( 2019122019193570.png) 

 For the specific interpretation of the above descriptions, we have organized group friends to read together and write their own understanding. I hope you can join us to learn and share. Interested friends can follow the WeChat official account, join the QQ or WeChat group, and communicate and share with you  



--------------------------------------------------------------------------------

This chapter mainly implements the relevant functions in the common file in PCL, which will be updated in the future. I hope to learn from each other and make progress together. 

 PCL-Common - Point cloud centroid solution 

 PCL-Common - Point Cloud Rotation 

 pcl-common篇-angel.h 

 pcl-common篇-color.h 

 pcl-common篇-common.h(1)) 

 pcl-common篇-common.h(2) 

 pcl-common篇-common.h(3) 

 pcl-common篇-copy_point.h 

 pcl-common篇-distance.h 

 ![avatar]( 20210628215843809.jpg) 

 J Build a group and hope to communicate together.  



--------------------------------------------------------------------------------

